{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Iris dataset\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Custom Decision Tree Implementation\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        # Stopping conditions\n",
    "        if len(unique_classes) == 1 or depth == self.max_depth:\n",
    "            return {\"class\": np.bincount(y).argmax()}\n",
    "\n",
    "        best_info_gain = -1\n",
    "        best_split = None\n",
    "        \n",
    "        # Find the best split\n",
    "        for feature_idx in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                if len(left_y) == 0 or len(right_y) == 0:\n",
    "                    continue\n",
    "\n",
    "                info_gain = self._information_gain(y, left_y, right_y)\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = info_gain\n",
    "                    best_split = {\n",
    "                        \"feature_idx\": feature_idx,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"left_mask\": left_mask,\n",
    "                        \"right_mask\": right_mask,\n",
    "                    }\n",
    "\n",
    "        if not best_split:\n",
    "            return {\"class\": np.bincount(y).argmax()}\n",
    "\n",
    "        # Recursively build the tree\n",
    "        left_tree = self._build_tree(X[best_split[\"left_mask\"]], y[best_split[\"left_mask\"]], depth + 1)\n",
    "        right_tree = self._build_tree(X[best_split[\"right_mask\"]], y[best_split[\"right_mask\"]], depth + 1)\n",
    "\n",
    "        return {\n",
    "            \"feature_idx\": best_split[\"feature_idx\"],\n",
    "            \"threshold\": best_split[\"threshold\"],\n",
    "            \"left_tree\": left_tree,\n",
    "            \"right_tree\": right_tree,\n",
    "        }\n",
    "\n",
    "    def _information_gain(self, parent, left, right):\n",
    "        parent_entropy = self._entropy(parent)\n",
    "        left_entropy = self._entropy(left)\n",
    "        right_entropy = self._entropy(right)\n",
    "\n",
    "        weighted_avg = (\n",
    "            (len(left) / len(parent)) * left_entropy + (len(right) / len(parent)) * right_entropy\n",
    "        )\n",
    "        return parent_entropy - weighted_avg\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        class_probs = np.bincount(y) / len(y)\n",
    "        return -np.sum(class_probs * np.log2(class_probs + 1e-9))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_single(x, self.tree) for x in X]\n",
    "\n",
    "    def _predict_single(self, x, tree):\n",
    "        if \"class\" in tree:\n",
    "            return tree[\"class\"]\n",
    "\n",
    "        feature_val = x[tree[\"feature_idx\"]]\n",
    "        if feature_val <= tree[\"threshold\"]:\n",
    "            return self._predict_single(x, tree[\"left_tree\"])\n",
    "        else:\n",
    "            return self._predict_single(x, tree[\"right_tree\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Decision Tree Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the custom decision tree\n",
    "custom_tree = CustomDecisionTree(max_depth=3)\n",
    "custom_tree.fit(X_train_iris, y_train_iris)\n",
    "y_pred_custom = custom_tree.predict(X_test_iris)\n",
    "accuracy_custom = accuracy_score(y_test_iris, y_pred_custom)\n",
    "print(f\"Custom Decision Tree Accuracy: {accuracy_custom:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Decision Tree Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 3. Scikit-learn Decision Tree\n",
    "sklearn_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "sklearn_tree.fit(X_train_iris, y_train_iris)\n",
    "y_pred_sklearn = sklearn_tree.predict(X_test_iris)\n",
    "accuracy_sklearn = accuracy_score(y_test_iris, y_pred_sklearn)\n",
    "print(f\"Scikit-learn Decision Tree Accuracy: {accuracy_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest F1 Score: 1.0000\n",
      "Decision Tree F1 Score: 0.9440\n"
     ]
    }
   ],
   "source": [
    "# 4. Ensemble Methods with the Wine Dataset\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "\n",
    "# Split the Wine dataset\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "rf_clf.fit(X_train_wine, y_train_wine)\n",
    "dt_clf.fit(X_train_wine, y_train_wine)\n",
    "\n",
    "rf_f1 = f1_score(y_test_wine, rf_clf.predict(X_test_wine), average=\"weighted\")\n",
    "dt_f1 = f1_score(y_test_wine, dt_clf.predict(X_test_wine), average=\"weighted\")\n",
    "\n",
    "print(f\"Random Forest F1 Score: {rf_f1:.4f}\")\n",
    "print(f\"Decision Tree F1 Score: {dt_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from GridSearchCV: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Parameters from RandomizedSearchCV: {'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "# 5. Hyperparameter Tuning\n",
    "# GridSearchCV for RandomForestClassifier\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, scoring=\"f1_weighted\")\n",
    "grid_search.fit(X_train_wine, y_train_wine)\n",
    "print(f\"Best Parameters from GridSearchCV: {grid_search.best_params_}\")\n",
    "\n",
    "# RandomizedSearchCV for RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "param_dist = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],\n",
    "    \"max_depth\": [None, 5, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "}\n",
    "random_search = RandomizedSearchCV(rf_reg, param_dist, n_iter=10, scoring=\"neg_mean_squared_error\", random_state=42)\n",
    "random_search.fit(X_train_wine, y_train_wine)\n",
    "print(f\"Best Parameters from RandomizedSearchCV: {random_search.best_params_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
